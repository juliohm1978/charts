# Values for Deployment/Statefulset/DaemonSet
deployment:

  ## Choose between Deployments, StatefulSets ou DaemonSets.
  ## Uncomment/recomment the blocks that you want to use.

  # Treat this as a Deployment
  kind: Deployment
  strategy: {}
  #   type: RollingUpdate  ## valores: RollingUpdate | Recreate
  #   rollingUpdate:
  #     maxSurge: 1
  #     maxUnavailable: 0

  # Treat this as a DaemonSet
  # kind: DaemonSet
  # strategy:
  #   type: RollingUpdate  ## valores: RollingUpdate | OnDelete
  #   rollingUpdate:
  #     maxUnavailable: 1

  # Treat this as a StatefulSet
  # kind: StatefulSet
  # podManagementPolicy: OrderedReady  ## valores: OrderedReady | Parallel
  # strategy:
  #   type: RollingUpdate  ## valores: RollingUpdate | OnDelete
  #   rollingUpdate:
  #     # WARNING! Review the meaning of `partition` to know what you are doing.
  #     # https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
  #     partition: 0

  replicaCount: 1

  labels: {}
    # label1: value1
    # label2: value2

  annotations: {}
    # annot1: "valor1"
    # annot2: "valor2"

  # Maximum time to wait for pods to reach state Ready
  progressDeadlineSeconds: 900

  # Maximum number of revisions in the history
  revisionHistoryLimit: 5

# Pod template configuration
pod:
  ## image and tag are MANDATORY
  image: ""
  tag: ""

  ## Instead of using `imagePullPolicy: Always`, consider passing
  ## the pod.digest value as an argument. The chart assumes the digest
  ## is calculated before hand, and will append it as `image:tag@digest`
  #digest: ""

  imagePullPolicy: IfNotPresent
  
  imagePullSecrets: []
    # - name: secret1
    # - name: secret2

  ## Uncomment if you need to create a ServiceAccount for the pod.
  ## Roles and RoleBindings can also be created here with the necessary
  ## permissions. If the account name is "default", only the roles and role
  ## bindings will be created.
  serviceAccount:
    create: false
    # name: my-svc-account-name
    # roleType: ClusterRole
    # bindingType: ClusterRoleBinding
    # rules:
    #   - apiGroups:
    #     - ""
    #     resources:
    #     - pods
    #     - namespaces
    #     verbs:
    #     - get
    #     - list
    #     - watch

  annotations: {}
    # annot1: "valor1"
    # annot2: "valor2"

  labels: {}
    # label1: value1
    # label2: value2

  ## Time to wait for graceful pod termination before Chuck Norris comes in.
  terminationGracePeriodSeconds: 5

  ## Enabling this adds a label called `idDeploy` to the pod template.
  ## The value is a random UUID, so it will change every time you run
  ## helm upgrade on the same release.
  ##
  ## In practice, this forces the pod to restart with every upgrade. Consider
  ## this a hack you can use this, along with `imagePullPolicy: Always` as a
  ## way to keep the containers fresh with the latest build.
  ##
  ## Keep in mind that using `latest` tag is not a recommended practice. There
  ## are other ways to update the containers without changing tags, such as
  ## using image digests, which this chart supports by the value `pod.digest`.
  enableIdDeploy: false

  ## Environment variables
  env: []
    # - name: VAR1
    #   value: "VAL1"
    # - name: VAR2
    #   value: "VAL2"

  ## The equivalent of ENTRYPOINT in a Dockerfile
  command: []
    # - /path/to/binary/inside/container
    # - arg1
    # - arg2

  ## The equivalent of CMD in a Dockerfile
  args: []
    # - arg3
    # - arg4
    # - arg5

  ## Readiness, Liveness, and Startup probes.
  ## Types: httpGet or tcpSocket.
  ##
  ## `path` and `port` are derived from the Ingress and Service,
  ## but they can be overriden.

  readinessProbe:
    enabled: true
    type: httpGet
    # path: /{{.Release.Name}}
    # port: {{.Values.service.ports[0]}}
    scheme: HTTP

    ## standard k8s values
    initialDelaySeconds: 0
    timeoutSeconds: 5
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 3

  livenessProbe:
    enabled: true
    type: httpGet
    # path: /{{.Release.Name}}
    # port: {{.Values.service.ports[0]}}
    scheme: HTTP

    ## standard k8s values
    initialDelaySeconds: 0
    timeoutSeconds: 5
    periodSeconds: 10
    successThreshold: 1
    failureThreshold: 3

  startupProbe:
    enabled: true
    type: httpGet
    # path: /{{.Release.Name}}
    # port: {{.Values.service.ports[0]}}
    scheme: HTTP

    ## wait up to 15 minutes
    initialDelaySeconds: 0
    timeoutSeconds: 3
    periodSeconds: 1
    successThreshold: 1
    failureThreshold: 900

  ## Pod security context, as defined in
  ## https://kubernetes.io/docs/tasks/configure-pod-container/security-context/  
  podSecurityContext: {}
    #sysctls:
    #  - name: kernel.shm_rmid_forced
    #    value: "0"
    #  - name: net.ipv4.route.min_pmtu
    #    value: "552"
    #  - name: kernel.msgmax
    #    value: "65536"

  ## Container security context
  securityContext: {}
    #privileged: true
    #capabilities:
    #  add: ["NET_ADMIN"]

  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  nodeSelector: {}

  tolerations: []

  affinity: {}

  ## Any other config you might want appended to the Pod template.
  extraPodConfig: {}
  
  ## Define initContainers that run before the Pod's containers start.
  ## https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
  initContainers: []
    # - name: initContainer1
    #   image: nginx
    #   imagePullPolicy: Always
    #   command:
    #     - /bin/sh
    #     - -c
    #     - while true; do
    #         date;
    #         sleep 30;
    #       done;

  ## Any extra containers you might want appended to the Pod template.
  ## These will not make any assumptions, all values must be hard-coded.
  extraContainers: []
    # - name: extraContainer1
    #   image: nginx
    #   imagePullPolicy: Always
    #   command:
    #     - /bin/sh
    #     - -c
    #     - while true; do
    #         date;
    #         sleep 30;
    #       done;

## Only one Service object is supported. For consistency, the chart maps
## all ports from this service into the main container of the Pod template.
service:
  type: ClusterIP
  ports: []
    # - name: http
    #   port: 80
    #   targetPort: 8080
    #   protocol: TCP
    #   nodePort: 30000
    
    # - name: https
    #   port: 443
    #   targetPort: 8443
    #   protocol: TCP
    # - name: porta1001
    #   port: 1001
    #   protocol: UDP

ingresses:

  ## Annotations that will be appended to all Ingresses.
  annotations: {}
    #kubernetes.io/ingress.class: nginx

  ## Defaults for all Ingresses. They can be overriden in each path declaration below.
  #domain: mydomain.com
  #tlsSecretName: mysecret1

  ## For the sake of simplicity, each path in the list will become a full Ingress object.
  paths: []
  # - name: app
  #   domain: another-domain.com
  #   path: /
  #   annotations: {}
  #   disableTLS: true
  #   tlsSecretName: mysecret2
  #   pathType: ImplementationSpecific
  #
  #   ## Optional.
  #   ## If not specified, the first port of the Service will be used.
  #   #servicePort: 9999
  #
  # - name: rest
  #   path: /api
  #   annotations:
  #     nginx.ingress.kubernetes.io/proxy-read-timeout: "300"

configMaps: []
  # - name: datasources
  #   mountPath: /environment
  #
  #   ## Enable this flag so that each entry in the ConfigMap mounts to a sepecific subPath.
  #   ## This helps if you do not want to overwrite the entire contents of a directory with
  #   ## the contents of the ConfigMap.
  #   mountSubPath: false (opcional, indica montagem das entradas do configMap como subPath)
  #
  #   data:
  #     pgDataSource.ds: |
  #       JNDINAME="java:/jdbc/myjndiname"
  #       USERNAME=user
  #       PASSWORD="password"
  #       URL="jdbc:Cache://remote-host:1972/DB_NAME"

  ## You can also create a ConfigMap that is not mounted or used as a Volume.
  ## Just do not specify a mountPath.
  # - name: myconfigmap
  #   data:
  #     LOCALE: pt_BR
  #     SOMEKEY: some_value

## The behavior or this PVC section depends on the deployment type.
## For Deployments, creates PVCs that are referenced as volumeMounts.
## For StatefulSets, creates volumeClaimTemplates that are referenced as volumeMounts.
persistentVolumeClaims: []
  # - name: www
  #   readOnly: false
  #   mountPath: /path/inside/container
  #   spec:
  #     accessModes: [ "ReadWriteOnce" ]
  #     storageClassName: "my-storage-class"
  #     resources:
  #       requests:
  #         storage: 1Gi

## This creates PVs directly in the Pod template.
## Sometimes, PVCs are just not necessary.
volumes: []
  # - name: myvolume
  #   mountPath: /path/inside/container
  #   spec:
  #     nfs:
  #       server: 123.123.123.123
  #       path: /NFS_server/share

  # - name: myvolume
  #   mountPath: /path/inside/container
  #   spec:
  #     flexVolume:
  #       driver: juliohm/cifs
  #       options:
  #         server: 192.168.99.112
  #         share: /mount
  #         passwdMethod: env
  #       secretRef:
  #         name: client-credentials

  # - name: myvolume
  #   mountPath: /path/inside/container
  #   spec:
  #     emptyDir: {}

  # - name: logs
  #   mountPath: /var/log
  #   spec:
  #     hostPath:
  #       path: /var/log

  # - name: env-config
  #   mountPath: /etc/env-config
  #   spec:
  #     configMap:
  #       name: env-config

  # - name: from-secret
  #   mountPath: /secret-location
  #   readOnly: true
  #   spec:
  #     secret:
  #       secretName: mysecret

## Mount extra paths from volumes that are not managed with this chart.
extraVolumeMounts: []
  # - mountPath: /etc/config/file
  #   name: config-map
  #   subPath: file

## Create NetworkPolicies.
## Label selectors are automatically adjusted for the pods in this deployment.
networkPolicies: []
  # - name: deny-all
  #   podSelector:
  #     function: "job"
  #   policyTypes:
  #     - Ingress
  #     - Egress
  # - name: deny-ingress
  #   policyTypes:
  #     - Ingress
  # - name: deny-egress
  #   policyTypes:
  #     - Egress
  # - name: permit-all
  #   policyTypes:
  #     - Ingress
  #     - Egress
  #   ingress:
  #     - {}
  #   egress:
  #     - {}
  # - name: permit-ingress
  #   policyTypes:
  #     - Ingress
  #   ingress:
  #     - {}
  # - name: permit-egress
  #   policyTypes:
  #     - Egress
  #   egress:
  #     - {}
  # - name: permit-ingress-http
  #   policyTypes:
  #     - Ingress
  #   ingress:
  #     - from:
  #       - ipBlock:
  #           cidr: 123.123.123.0/24
  #       ports:
  #         - protocol: TCP
  #           port: 80
  # - name: permit-egress-psql
  #   policyTypes:
  #     - Egress
  #   egress:
  #     - to:
  #       - ipBlock:
  #           cidr: 123.123.123.0/24
  #       ports:
  #         - protocol: TCP
  #           port: 5432
  # - name: permit-by-labels
  #   policyTypes:
  #     - Egress
  #     - Ingress
  #   egress:
  #     - to:
  #       - namespaceSelector:
  #           matchLabels:
  #             somelabel: labelvalue
  #   ingress:
  #     - from:
  #       - podSelector:
  #           matchLabels:
  #             role: frontend

## The following `prometheusOperator` section should only be used if you
## already have a PrometheusOperator stack installed in the cluster.
## 
## Keep in mind that PrometheusOperator already deploys general purpose
## alerts for cpu, memory, network, and several other basic cluster metrics.
##
## Normally, you will use this to define custom collection endpoints that
## provide further metrics beyond that basic.
##
## You can also use it to create Rules and Alerts.

## Check out the docs for the Operator API
## https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md
##
## The values here are ALMOST a mirror of the plain Prometheus configuration.
## https://prometheus.io/docs/alerting/latest/configuration/#receiver
##
## Be careful to replace the Prometheus config names using a Camel Case standard
## https://en.wikipedia.org/wiki/Camel_case
## That means: "email_configs" becomes "emailConfigs".
##
prometheusOperator:

  ## Define ServiceMonitors to create collection points.
  ## https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#servicemonitorspec
  ## Label selectors will be automatically added to restrict the collection to your application.
  ##
  ## IMPORTANT: Ports used in the ServiceMonitor NEED to be declared in the Pod containers.
  ##            For this chart, make sure you declare them in the `service`.
  serviceMonitors: []
    # - name: mymetrics
    #   endpoints:
    #     ## Each ServiceMonitor can group a number of collection Endpoints.
    #     ## https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#endpoint
    #     - targetPort: 9404
    #       path: /metrics
    #       scheme: http
    #       params: {}
    #       interval: 30s
    #       scrapeTimeout: 15s

  ## Define alerting rules. They will be directed to the receivers declared below.
  ## For simplicity, the label `receiver` can be used to direct an alert to a specific
  ## receiver from your list. Otherwise, alerts are directed to the default (1st on the list).
  ##
  ## Alertas que serão direcionados aos receivers desta aplicação.
  ## Para facilidade, o label "receiver" pode ser usado para indicar para onde
  ## o alerta deve ser enviado. Sem labels, o alerta será direcionado ao receiver
  ## padrão das rotas abaixo.
  ##
  ## For advanced rules, create your labels that can be used to route to different receivers.
  ##
  ## WARNING: Alert names CANNOT have spaces. They need to follow the naming rules of a Kubernetes resource.
  rules: []
    # - alert: AlertToDiscord
    #   expr: |
    #     count(kube_deployment_created) < 0
    #   annotations:
    #     summary: This alert will go to a discord receiver
    #     description: ""
    # - alert: AlertToEmail
    #   expr: |
    #     count(kube_deployment_created) < 0
    #   labels:
    #     receiver: email
    #   annotations:
    #     summary: This alert will go to an email receiver
    #     description: ""

  ## Define receivers for the alerts you created above.
  ## For simplicity, you can define a single receiver. A default
  ## route is created to match all the alerts in your list.
  ##
  ## If you define more than one receiver, the first one will
  ## be the default. You can change the default receiver with:
  ##
  ##    route:
  ##      receiver: nome-do-receiver
  ##
  ## By default, all alerts in your list will be routed to the default
  ## receiver. The change that behavior, include a label in the
  ## alert that informs a different receiver:
  ##
  ##    receiver: name-of-receiver
  ## 
  route:
    ## Change the default receiver, if you don't want it to be the
    ## first one of your list.
    # receiver: email

    ## Customize intervals. If not defined, the global ones defined
    ## in your Alertamanger will be used.
    # groupBy: ["alertname"]
    # groupWait: 30s
    # groupInterval: 10s
    # repeatInterval: 1m
    
    ## CUIDADO: This is not working for some reason!
    ## The value will be added, but I guess it does not work as I expected.
    # continue: "false"

    ## Define sub-routes, only if you need more advanced routing rules
    ## based on your own custom labels.
    # routes:
    #   - matchers:
    #       - name: area
    #         value: financeiro
    #     receiver: email
    #     groupBy: ["alertname"]
    #     groupWait: 30s
    #     groupInterval: 10s
    #     repeatInterval: 1m

    ## Check out the Receiver API
    ## https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#receiver
    ##
    ## Keep in mind that the format is different from the standard Prometheus config,
    ## especially for email headers.
    ##
    receivers: []
      # - name: canalDiscord
      #   webhookConfigs:
      #     - url: 'http://discordbot.botnamespace:9095'
      # - name: email
      #   emailConfigs:
      #   - to: 'email@example.com'
      #     sendResolved: true
      #     html: '{{ template "email.body.html" . }}'
      #     headers:
      #       - key: Subject
      #         value: '{{ template "email.subject.html" . }}'
